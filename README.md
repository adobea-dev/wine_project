<<<<<<< HEAD
## ğŸ§  Wine Quality Prediction â€“ End-to-End ML System
=======
ğŸ§  Wine Quality Prediction â€“ End-to-End ML System
>>>>>>> ce765af5 (multi class)

A fully modular machine learning pipeline for multiclass wine quality classification (Low, Medium, High).
This project demonstrates a production-ready ML workflow â€” from data ingestion and feature engineering to model evaluation, persistence, and REST API deployment.

ğŸš€ Highlights

Automated ML pipeline (data cleaning â†’ feature engineering â†’ model training â†’ evaluation)

Reproducible experiments via YAML configs & runtime logging

Model comparison across Logistic Regression, Random Forest, SVM, XGBoost, and more

Explainable metrics â€“ macro/micro/weighted F1, precision/recall, confusion matrix, ROC (OVR)

FastAPI deployment exposing a /predict endpoint

Config-driven runs: adjustable preprocessing, sampling, and model parameters

ğŸ“ Project Structure
wine_quality_project/
â”œâ”€â”€ bin/
â”‚   â””â”€â”€ run_pipeline.sh                # One-command reproducible run
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml                   # Main config for pipeline
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                           # Original dataset
â”‚   â””â”€â”€ processed/                     # Cleaned and labeled data
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ champion.joblib                # Best performing model
â”‚   â”œâ”€â”€ champion_metadata.json         # Model metadata
â”‚   â””â”€â”€ scaler.pkl                     # Scaler used in preprocessing
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ runs/                          # Logged runs (metrics, configs, plots)
â”‚   â”œâ”€â”€ metrics/                       # Evaluation metrics JSONs
â”‚   â”œâ”€â”€ figures/                       # Confusion matrices, ROC curves, etc.
â”‚   â””â”€â”€ runtime/                       # Environment snapshot
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                           # FastAPI app for deployment
â”‚   â”œâ”€â”€ pipeline/                      # Orchestration (main.py)
â”‚   â”œâ”€â”€ data_management/               # Downloading, cleaning, validation
â”‚   â”œâ”€â”€ features/                      # Feature engineering & scaling
â”‚   â”œâ”€â”€ models/                        # Training, evaluation, comparison
â”‚   â”œâ”€â”€ utils/                         # I/O, runtime info, tracking
â”‚   â””â”€â”€ visualization/                 # EDA plots
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements.lock.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

<<<<<<< HEAD

ğŸ§© Workflow Overview
1ï¸âƒ£ Data Management

Downloads or loads dataset (winequality-red.csv, winequality-white.csv)

Cleans missing values, fixes types, removes outliers

Converts wine quality scores to multiclass categories (Low / Medium / High)

2ï¸âƒ£ Feature Engineering

=======
ğŸ§© Workflow Overview
1ï¸âƒ£ Data Management

Downloads or loads dataset (winequality-red.csv, winequality-white.csv)

Cleans missing values, fixes types, removes outliers

Converts wine quality scores to multiclass categories (Low / Medium / High)

2ï¸âƒ£ Feature Engineering

>>>>>>> ce765af5 (multi class)
Polynomial and interaction feature generation

Feature selection via correlation or statistical tests

Scaling via StandardScalerWrapper

Optional SMOTE for class balance

3ï¸âƒ£ Model Training & Evaluation

Trains multiple models (Logistic Regression, SVM, RandomForest, GradientBoosting, XGBoost, LightGBM)

Evaluates using:

Accuracy

Macro/Weighted F1

Precision/Recall per class

Confusion matrix

ROC (One-vs-Rest)

4ï¸âƒ£ Model Comparison

src/models/model_comparator.py ranks models by f1_weighted

Saves metrics & charts (reports/metrics, reports/figures)

5ï¸âƒ£ Experiment Tracking

Auto-saves:

runtime_info.json â†’ Python, library versions, OS, timestamp

config_used.yaml â†’ parameters used for that run

metrics/*.json â†’ all performance outputs

6ï¸âƒ£ Deployment

FastAPI app serves predictions:

uvicorn src.api.app:app --reload


Swagger docs: http://127.0.0.1:8000/docs

âš™ï¸ Reproducible Run

Run the full pipeline and log everything automatically:

bash bin/run_pipeline.sh


Or manually:

python -m src.pipeline.main --config configs/default.yaml --outdir reports/runs/local


Results, metrics, and plots will appear under:

reports/runs/<timestamp>/

ğŸŒ API Usage

After running:

uvicorn src.api.app:app --reload


Visit:

http://127.0.0.1:8000/docs


Example Request:

{
  "fixed_acidity": 7.4,
  "volatile_acidity": 0.70,
  "citric_acid": 0.00,
  "residual_sugar": 1.9,
  "chlorides": 0.076,
  "free_sulfur_dioxide": 11.0,
  "total_sulfur_dioxide": 34.0,
  "density": 0.9978,
  "pH": 3.51,
  "sulphates": 0.56,
  "alcohol": 9.4
}


Response:

{
  "prediction": "Medium",
  "probabilities": {"Low": 0.10, "Medium": 0.80, "High": 0.10},
  "model": "champion.joblib",
  "timestamp": "2025-10-07T16:22Z"
}

ğŸ§ª Evaluation Summary
Metric	Best Model	Accuracy	Macro F1	Weighted F1
RandomForest	âœ… Champion	0.89	0.87	0.88

(Values shown as example â€” replace with your real evaluation output from reports/metrics/test_metrics_champion.json.)

ğŸ§° Tech Stack

Python 3.12+

Pandas, NumPy, scikit-learn

Imbalanced-Learn (SMOTE)

XGBoost, LightGBM

Matplotlib, Seaborn

FastAPI, Uvicorn

Joblib, YAML, JSON

ğŸ“¦ Environment Setup
# Create environment
python -m venv winequality
.\winequality\Scripts\activate   # (Windows)
# source winequality/bin/activate  (Linux/Mac)

# Install dependencies
pip install -r requirements.txt


Freeze the exact environment after a successful run:

pip freeze > requirements.lock.txt

ğŸ§¾ Configuration Example (configs/default.yaml)
random_state: 42
test_size: 0.2
val_size: 0.2
target_col: quality_category

preprocess:
  create_interactions: true
  feature_selection_method: correlation
<<<<<<< HEAD
 
=======
  k_top: 8
  smote:
    enabled: true
    sampling_strategy: auto
>>>>>>> ce765af5 (multi class)

models:
  logistic_regression:
    enabled: true
  random_forest:
    enabled: true
  svm:
    enabled: true

ğŸ§  Future Work

Add SHAP explainability & feature importance visualizations

Extend FastAPI endpoints to include /metadata and /retrain

Integrate MLflow for experiment management

Deploy on Docker or Azure Container Apps

ğŸ‘©ğŸ½â€ğŸ’» Author

Regina Adobea Essien
<<<<<<< HEAD
MSc Data Science Researcher | Ghana Data Science Community
ğŸ“§ reginaessien83@gmail.com

ğŸ”— LinkedIn
 â€¢ GitHub
=======
MSc Data Science 
ğŸ“§ reginaessien83@gmail.com

ğŸ”— LinkedIn
 â€¢ GitHub
>>>>>>> ce765af5 (multi class)
